{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqXsJVxUN5KW"
   },
   "source": [
    "# LTH Implementation Conv2 - SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gz-8MdUoN5Kh"
   },
   "outputs": [],
   "source": [
    "# parts from \"Conv_2_CIFAR10_Magnitude_based_Pruning_Gaussian_Glorot_initializations\", \n",
    "# https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35iQxSYTN5Kk"
   },
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nDpAVmhN5Km",
    "outputId": "80e86cba-1ed3-4057-9320-369116f5814f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.layers import AveragePooling2D, Conv2D, MaxPooling2D, ReLU\n",
    "from tensorflow.keras import models, layers, datasets\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, InputLayer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMgcs85XN5Ko"
   },
   "source": [
    "## Load and handle SVHN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBv78zRuN5Kp"
   },
   "outputs": [],
   "source": [
    "# save important features of SVHN \n",
    "num_classes = 10\n",
    "img_rows, img_cols = 32, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "################# Change Folder here! ##############################\n",
    "# SVHN data can be found here: http://ufldl.stanford.edu/housenumbers\n",
    "train = loadmat(\"data/numbers_train_32x32.mat\")\n",
    "test = loadmat(\"data/numbers_test_32x32.mat\")\n",
    "\n",
    "X_train = np.array(train[\"X\"])\n",
    "y_train = np.array(train[\"y\"])\n",
    "\n",
    "X_test = np.array(test[\"X\"])\n",
    "y_test = np.array(test[\"y\"])\n",
    "\n",
    "# bring into right format (shape = (nr_images, height, width, channels))\n",
    "X_train = np.swapaxes(X_train, 3, 0)\n",
    "X_train = np.swapaxes(X_train, 3, 1)\n",
    "X_train = np.swapaxes(X_train, 3, 2)\n",
    "\n",
    "X_test = np.swapaxes(X_test, 3, 0)\n",
    "X_test = np.swapaxes(X_test, 3, 1)\n",
    "X_test = np.swapaxes(X_test, 3, 2)\n",
    "\n",
    "# change label '10' to '0'\n",
    "y_train = np.array([x if x != [10] else [0] for x in y_train])\n",
    "y_test = np.array([x if x != [10] else [0] for x in y_test])\n",
    "\n",
    "# Convert datasets to floating point types-\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize the training and testing datasets-\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "# convert class vectors/target to binary class matrices or one-hot encoded values-\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# split training set into train and validation (10%)\n",
    "l = len(X_train)\n",
    "X_val = X_train[:int(l/10)]\n",
    "X_train = X_train[int(l/10):]\n",
    "\n",
    "y_val = y_train[:int(l/10)]\n",
    "y_train = y_train[int(l/10):]\n",
    "\n",
    "# combine into real datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds = train_ds.batch(batch_size = batch_size)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_ds = val_ds.batch(batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj9uOsyuiHUo"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFa6CZOJiU-D"
   },
   "source": [
    "### Conv-2 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRmVk_yxY3RB"
   },
   "outputs": [],
   "source": [
    "# oriented at https://keras.io/guides/customizing_what_happens_in_fit/, last retrieved 15.12.21\n",
    "class CustomModel(keras.Sequential):\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        x, y = data\n",
    "                        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # List to hold element-wise multiplication between-\n",
    "        # computed gradient and masks-\n",
    "        grad_mask_mul = []\n",
    "        \n",
    "\n",
    "        # Perform element-wise multiplication between computed gradients and 0, if weight was already\n",
    "        # 0 before (was pruned, very unlikely that a weight would by chance reach exactly 0!), gradient is \n",
    "        # multiplied by 1 else (in order to keep the pruned weights at 0, apply the gradient everywhere else\n",
    "        for grad_layer, train_vars in zip(gradients, trainable_vars):\n",
    "            grad_mask_mul.append(tf.math.multiply(grad_layer, tf.math.ceil(tf.math.abs(train_vars))))\n",
    "    \n",
    "        # Apply computed gradients to model's weights and biases-\n",
    "        self.optimizer.apply_gradients(zip(grad_mask_mul, trainable_vars))\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5-78FddN5Kz"
   },
   "outputs": [],
   "source": [
    "def conv2_cnn(initializer = tf.initializers.GlorotNormal(), custom = False):\n",
    "    \"\"\"\n",
    "    Function to define the architecture of a neural network model\n",
    "    following Conv-2 architecture for CIFAR-10 dataset and using\n",
    "    provided parameter which are used to prune the model.\n",
    "\n",
    "    Conv-2 architecture-\n",
    "    64, 64, pool  -- convolutions\n",
    "    256, 256, 10  -- fully connected layers\n",
    "\n",
    "    Inputs:\n",
    "    'initializer' defines which initializer to use for weights and biases\n",
    "    'custom' is True if model should follow the custom fit-function and not train weights that are 0\n",
    "\n",
    "    Output: Returns designed and compiled neural network model\n",
    "    \"\"\"\n",
    "    if custom:\n",
    "        model = CustomModel()\n",
    "    else:\n",
    "        model = Sequential()\n",
    "    \n",
    "    # first conv layer\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            filters = 64, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = initializer,\n",
    "            bias_initializer = initializer,\n",
    "            strides = (1, 1), padding = 'same',\n",
    "            input_shape=(32,32, 3)\n",
    "            )\n",
    "    )\n",
    "\n",
    "    # second conv layer\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            filters = 64, kernel_size = (3, 3),\n",
    "            activation='relu', kernel_initializer = initializer,\n",
    "            bias_initializer = initializer,\n",
    "            strides = (1, 1), padding = 'same'\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    # pooling layer\n",
    "    model.add(\n",
    "        MaxPooling2D(\n",
    "            pool_size = (2, 2),\n",
    "            strides = (2, 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # first dense layer\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units = 256, activation='relu',\n",
    "            kernel_initializer = initializer,\n",
    "            bias_initializer = initializer\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # second dense layer\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units = 256, activation='relu',\n",
    "            kernel_initializer = initializer,\n",
    "            bias_initializer = initializer\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # third dense layer / output layer\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units = 10, activation='softmax'\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    # Compile CNN-\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0002),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcGBshv8jIan"
   },
   "source": [
    "### Plot Histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F42O6ClGjJuh"
   },
   "outputs": [],
   "source": [
    "def plot_histories(his1,name1,his2,name2):\n",
    "    plt.plot(his1.history[\"accuracy\"], label = \"accuracy {0}\".format(name1))\n",
    "    plt.plot(his2.history[\"accuracy\"], label = \"accuracy {0}\".format(name2))\n",
    "    plt.plot(his1.history[\"val_accuracy\"], label = \"val_accuracy {0}\".format(name1))\n",
    "    plt.plot(his2.history[\"val_accuracy\"], label = \"val_accuracy {0}\".format(name2))\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.plot(his1.history[\"loss\"], label = \"loss {0}\".format(name1))\n",
    "    plt.plot(his2.history[\"loss\"], label = \"loss {0}\".format(name2))\n",
    "    plt.plot(his1.history[\"val_loss\"], label = \"val_loss {0}\".format(name1))\n",
    "    plt.plot(his2.history[\"val_loss\"], label = \"val_loss {0}\".format(name2))\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tAIU8t6kIoQ"
   },
   "source": [
    "### Prune Conv-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQZ84C5kN5LA"
   },
   "outputs": [],
   "source": [
    "# from arjun, changed to give back model and not give back pruned weights in list\n",
    "\n",
    "def prune_conv2(model, pruning_params_conv, pruning_params_fc, pruning_params_op):\n",
    "    '''\n",
    "    Function to prune top p% of trained weights using the provided parameters using\n",
    "    magnitude-based weight pruning.\n",
    "    \n",
    "    Inputs:\n",
    "    'model' is the TensorFlow 2.0 defined convolutional neural network\n",
    "    'pruning_params_conv' is the percentage of weights to prune for convolutional layer\n",
    "    'pruning_params_fc' is the percentage of weights to prune for dense, fully-connected layer\n",
    "    'pruning_params_op' is the percentage of weights to prune for output layer\n",
    "\n",
    "    Returns:\n",
    "    pruned_model \n",
    "    '''\n",
    "\n",
    "    ### initialize new model with 0 everywhere that's going to get changed\n",
    "    pruned_model = conv2_cnn(\"zeros\", custom = True)  \n",
    "\n",
    "    for layer_old, layer_pruned in zip(model.layers, pruned_model.layers):   \n",
    "        \n",
    "        # if max_pooling or flatten\n",
    "        if layer_old.get_weights() == []:\n",
    "            continue\n",
    "        \n",
    "        # if convolutional layer\n",
    "        if len(layer_old.get_weights()[0].shape) == 4:\n",
    "            \n",
    "            #print(\"conv layer: {0}, pruning rate = {1}%\".format(layer_old.get_weights()[0].shape, pruning_params_conv))\n",
    "            \n",
    "            # shape of w is (height, width, channels, tensors), shape of b is (tensors)\n",
    "            w = layer_old.get_weights()[0]\n",
    "            b = layer_old.get_weights()[1] # doesn't need to be changed\n",
    "            \n",
    "            # Compute absolute value of 'w'\n",
    "            w_abs = np.abs(w)\n",
    "\n",
    "            # Mask values to zero which are less than 'p' in terms of magnitude\n",
    "            w_abs[w_abs < np.percentile(w_abs, pruning_params_conv)] = 0\n",
    "\n",
    "            # Where 'w_abs' equals 0, keep 0, else, replace with values of 'w'\n",
    "            w_new = np.where(w_abs == 0, 0, w)\n",
    "    \n",
    "            layer_pruned.set_weights([w_new,b])\n",
    "\n",
    "        # if fully-connected dense layer\n",
    "        elif len(layer_old.get_weights()[0].shape) == 2 and layer_old.get_weights()[0].shape[1] != 10:\n",
    "\n",
    "            #print(\"dense layer: {0}, pruning rate = {1}%\".format(layer_old.get_weights()[0].shape, pruning_params_fc))\n",
    "            \n",
    "            # shape of w is (incoming_image_pixels, neurons), shape of b is (neurons)\n",
    "            w = layer_old.get_weights()[0]\n",
    "            b = layer_old.get_weights()[1] # doesn't need to be changed\n",
    "\n",
    "            # Compute absolute value of 'w'\n",
    "            w_abs = np.abs(w)\n",
    "\n",
    "            # Mask values to zero which are less than 'p' in terms of magnitude\n",
    "            w_abs[w_abs < np.percentile(w_abs, pruning_params_fc)] = 0\n",
    "\n",
    "            # Where 'w_abs' equals 0, keep 0, else, replace with values of 'w'\n",
    "            w_new = np.where(w_abs == 0, 0, w)\n",
    "\n",
    "            layer_pruned.set_weights([w_new,b])\n",
    "        \n",
    "        # if output layer\n",
    "        elif len(layer_old.get_weights()[0].shape) == 2 and layer_old.get_weights()[0].shape[1] == 10:\n",
    "\n",
    "            #print(\"op layer: {0}, pruning rate = {1}%\".format(layer_old.get_weights()[0].shape, pruning_params_op))\n",
    "\n",
    "            # shape of w is (incoming_image_pixels, neurons), shape of b is (neurons)\n",
    "            w = layer_old.get_weights()[0]\n",
    "            b = layer_old.get_weights()[1] # doesn't need to be changed\n",
    "\n",
    "            # Compute absolute value of 'w'\n",
    "            w_abs = np.abs(w)\n",
    "\n",
    "            # Mask values to zero which are less than 'p' in terms of magnitude\n",
    "            w_abs[w_abs < np.percentile(w_abs, pruning_params_fc)] = 0\n",
    "\n",
    "            # Where 'w_abs' equals 0, keep 0, else, replace with values of 'w'\n",
    "            w_new = np.where(w_abs == 0, 0, w)\n",
    "\n",
    "            layer_pruned.set_weights([w_new,b])\n",
    "\n",
    "\n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set IM-Pruning params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### set final pruning rate here !! ###########################################\n",
    "final_percentage_weights = 0.1  # at most 10% of weights should be left\n",
    "\n",
    "# number of convolutional parameters\n",
    "conv1 = 1792\n",
    "conv2 = 36928\n",
    "\n",
    "# number of fully-connected dense parameters\n",
    "dense1 = 4194560\n",
    "dense2 = 65792\n",
    "op_layer = 2570\n",
    "\n",
    "\n",
    "# total number of parameters\n",
    "total_params = conv1 + conv2 + dense1 + dense2 + op_layer\n",
    "\n",
    "final_pruned_params = final_percentage_weights * total_params \n",
    "\n",
    "# Lists to hold percentage of weights pruned in each round for all layers in CNN\n",
    "conv_pruning = []\n",
    "dense_pruning = []\n",
    "op_layer_pruning = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count necessary rounds of pruning \n",
    "n = 0\n",
    "\n",
    "# save momentary percentage of pruned params after n rounds\n",
    "loc_tot_params = total_params\n",
    "loc_conv1 = conv1\n",
    "loc_conv2 = conv2\n",
    "loc_dense1 = dense1\n",
    "loc_dense2 = dense2\n",
    "loc_op_layer = op_layer\n",
    "\n",
    "# save pruned percentages for each round until goal is reached\n",
    "while loc_tot_params >= final_pruned_params:\n",
    "    loc_conv1 *= 0.9    # 10% weights are pruned each round\n",
    "    loc_conv2 *= 0.9    # 10% weights are pruned\n",
    "    loc_dense1 *= 0.8   # 20% weights are pruned\n",
    "    loc_dense2 *= 0.8   # 20% weights are pruned\n",
    "    loc_op_layer *= 0.9 # 10% weights are pruned\n",
    "    \n",
    "    conv_pruning.append(((conv1 - loc_conv1) / conv1) * 100)\n",
    "    dense_pruning.append(((dense1 - loc_dense1) / dense1) * 100)\n",
    "    op_layer_pruning.append(((op_layer - loc_op_layer) / op_layer) * 100)\n",
    "\n",
    "    loc_tot_params = loc_conv1 + loc_conv2 + loc_dense1 + loc_dense2 + loc_op_layer\n",
    "\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from list to np.array and round\n",
    "conv_pruning = np.round(np.array(conv_pruning),3)\n",
    "dense_pruning = np.round(np.array(dense_pruning),3)\n",
    "op_layer_pruning = np.round(np.array(op_layer_pruning),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Magnitude Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each round of training before pruning, add history section to whole history\n",
    "def add_section_to_his(his_whole, his_section):\n",
    "    his_whole.history[\"accuracy\"] += his_section.history[\"accuracy\"]\n",
    "    his_whole.history[\"val_accuracy\"] += his_section.history[\"val_accuracy\"]\n",
    "    his_whole.history[\"loss\"] += his_section.history[\"loss\"]\n",
    "    his_whole.history[\"val_loss\"] += his_section.history[\"val_loss\"]\n",
    "    return his_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for both WT and original network, train for some epochs and save history to be able to compare \n",
    "# their performance later on\n",
    "def compare_WT_to_orig(WT, orig_model, nr):\n",
    "    # how many epochs they should train \n",
    "    # (about 40 to match the 6000 iterations of Frankle/Carbin, but 10 are enough)\n",
    "    epochs = 10\n",
    "\n",
    "    his_WT = WT.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        verbose = 0\n",
    "        )\n",
    "    \n",
    "    his_orig = orig_model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        verbose = 0\n",
    "    )\n",
    "    \n",
    "    # save histories\n",
    "    with open('../tickets/WTs_SVHN/his_WT_s0.1_nr' + str(nr), 'wb') as file_pi:\n",
    "        pickle.dump(his_WT.history, file_pi)\n",
    "        \n",
    "    with open('../tickets/WTs_SVHN/his_orig_s0.1_nr' + str(nr), 'wb') as file_pi:\n",
    "        pickle.dump(his_orig.history, file_pi)\n",
    "        \n",
    "    plot_histories(his_WT, \"WT\", his_orig, \"Orig\")\n",
    "    \n",
    "    # use early stopping criterion from Frankle/Carbin: Minimum val_loss iteration.\n",
    "    # only print results here, check later on when actually using the tickets\n",
    "    print(\"WT has min in epoch\", np.argmin(his_WT.history[\"val_loss\"])+1, \"with val_loss\", np.min(his_WT.history[\"val_loss\"]))\n",
    "          \n",
    "    print(\"Orig_model has min in epoch\", np.argmin(his_orig.history[\"val_loss\"])+1, \"with val_loss\", np.min(his_orig.history[\"val_loss\"]))\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### set parameters here ##################\n",
    "nr_winning_tickets = 20\n",
    "nr_pruning_rounds = n\n",
    "nr_epochs_per_round = 8\n",
    "\n",
    "# for each WT that should be extracted:\n",
    "for i in range(0, nr_winning_tickets):   \n",
    "    \n",
    "    print(\"Starting nr \", i)\n",
    "    \n",
    "    # initiate random network and save it\n",
    "    orig_model = conv2_cnn()\n",
    "    # serialize weights to HDF5\n",
    "    orig_model.save_weights(\"../tickets/WTs_SVHN/orig_model_s0.1_nr\" + str(i) + \".h5\")\n",
    "        \n",
    "    # train several steps, prune certain percentage, reinitialize and train again until final percentage achieved\n",
    "    # initiate model to be trained\n",
    "    trained_model = conv2_cnn(custom = True)\n",
    "    trained_model.set_weights(orig_model.get_weights()) \n",
    "        \n",
    "    for j in range(nr_pruning_rounds):\n",
    "        print(\"pruning round nr \", j+1, \"/\", nr_pruning_rounds)\n",
    "        # train for nr_epochs_per_round steps\n",
    "        # fit model and save accuracy in history\n",
    "        history_trained_net_section = trained_model.fit(\n",
    "            train_ds,\n",
    "            validation_data = val_ds,\n",
    "            epochs=nr_epochs_per_round\n",
    "            #verbose = 0, \n",
    "        )\n",
    "        \n",
    "        \n",
    "        # save history if its first round of pruning, add to existing history if not \n",
    "        if j == 0:\n",
    "            history_trained_net = history_trained_net_section\n",
    "        else:\n",
    "            history_trained_net = add_section_to_his(history_trained_net, history_trained_net_section)\n",
    "            \n",
    "            \n",
    "        # prune net according to pruning rates for this round\n",
    "        trained_model = prune_conv2(trained_model, conv_pruning[j], dense_pruning[j], op_layer_pruning[j])\n",
    "        \n",
    "        \n",
    "        # reinitialize pruned net\n",
    "        for orig_wts, trained_wts in zip(orig_model.trainable_weights, trained_model.trainable_weights):\n",
    "            trained_wts.assign(tf.where(tf.equal(trained_wts, 0), trained_wts, orig_wts))\n",
    "            \n",
    "\n",
    "    # final network is already a canditat for a WT as it was reinitialized, save it\n",
    "    trained_model.save_weights(\"../tickets/WTs_SVHN/WT_s0.1_nr\" + str(i) + \".h5\")\n",
    "    \n",
    "    # save history_trained_net\n",
    "    with open('../tickets/WTs_SVHN/his_train_0.1_nr' + str(i), 'wb') as file_pi:\n",
    "        pickle.dump(history_trained_net.history, file_pi)\n",
    "    \n",
    "    # check if WT (save histories to make it possible to check and decide later on)\n",
    "    compare_WT_to_orig(trained_model, orig_model, nr = i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at example history while IMP\n",
    "plot_history(history_trained_net)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UFa6CZOJiU-D",
    "QcGBshv8jIan",
    "xiKdNUfoj2hg",
    "4tAIU8t6kIoQ",
    "jYwuJLawN5Ky"
   ],
   "name": "LTH self implementation conv2_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
