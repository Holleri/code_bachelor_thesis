{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isabe\\Miniconda3\\envs\\cv\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\isabe\\Miniconda3\\envs\\cv\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\isabe\\Miniconda3\\envs\\cv\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SaSD local search approach to compare (Convolutional) Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Distance-Score without changing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeightsArray(mod):\n",
    "    weights = []\n",
    "    for layer in mod.layers:\n",
    "        if isinstance(layer, keras.layers.core.Dense) or isinstance(layer, keras.layers.convolutional.Conv2D):\n",
    "            weights.append(np.array(layer.get_weights()[0])) \n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets two vectors with all weights (! and not just the connection indices) and gives back edit distance\n",
    "def editDistanceSigns(a,b):\n",
    "    nums = len(a) - len(np.intersect1d(np.where(b==0), np.where(a==0)))\n",
    "    same = len(np.intersect1d(np.where(a<0), np.where(b<0))) + len(np.intersect1d(np.where(a>0), np.where(b>0)))\n",
    "    if nums == 0:\n",
    "        return 0\n",
    "    return (nums-same)/nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScoreLayers(layer1, layer2):\n",
    "    \n",
    "    assert layer1.shape == layer2.shape\n",
    "\n",
    "    # k is nr of kernels/neurons\n",
    "    k = layer1.shape[-1]\n",
    "    \n",
    "    signDist = 0\n",
    "        \n",
    "    # if conv layer, shape has length 4 (height, width, channels, kernels)\n",
    "    if len(layer1.shape) == 4:\n",
    "                \n",
    "        for kernel in range(k):\n",
    "            signDist += editDistanceSigns(layer1[:,:,:, kernel].flatten(), layer2[:,:,:, kernel].flatten())\n",
    "\n",
    "    \n",
    "    # a dense layer, shape is (neurons last layer, neurons this layer)\n",
    "    else:\n",
    "\n",
    "        for neuron in range(k):\n",
    "            signDist += editDistanceSigns(layer1[:, neuron], layer2[:, neuron])\n",
    "            \n",
    "    \n",
    "    return signDist/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns distance score between two models (without interchanging order of neurons)\n",
    "def getScoreModels(mod1, mod2):\n",
    "    \n",
    "    # get array of weights of conv and dense layers\n",
    "    weightsNN1 = getWeightsArray(mod1)\n",
    "    weightsNN2 = getWeightsArray(mod2)\n",
    "    \n",
    "    # boolean is set to True if we have a conv layer and remains as such until we reach first Dense layer\n",
    "    # it recognizes this and is set to False thereafter\n",
    "    firstDenseAfterConv = False\n",
    "    # saves length of last conv layer before first dense\n",
    "    lastConvLen = 0\n",
    "            \n",
    "    numLayers = len(weightsNN1)\n",
    "    assert len(weightsNN2) == numLayers\n",
    "    \n",
    "    editDistance = np.zeros(numLayers)\n",
    "    \n",
    "    # for all layers:\n",
    "    for k in range(numLayers):\n",
    "        layerNN1 = weightsNN1[k].copy()\n",
    "        layerNN2 = weightsNN2[k].copy()\n",
    "                \n",
    "        editDistance[k] = getScoreLayers(layerNN1, layerNN2)\n",
    "        \n",
    "    return editDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to change 2 kernels/neurons (neighbourhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_neurons(mod2, layer, n1, n2):\n",
    "    # initialize new (empty) model\n",
    "    mod2_new = getModelFromFile(\"../tickets/conv2.json\", \"../tickets/zeros.h5\")\n",
    "    # get weights from original one\n",
    "    weights_new = mod2.get_weights()\n",
    "    weights_old = mod2.get_weights()\n",
    "    \n",
    "   # change important weights\n",
    "    # if conv layer (shape = (height, width, channel, kernel))\n",
    "    if layer == 0:\n",
    "        # change kernels n1 and n2 (no need to change biases, \n",
    "        # they are not counting towards SaSD and were not pruned)\n",
    "        weights_new[0][:,:,:,n1] = weights_old[0][:,:,:,n2]\n",
    "        weights_new[0][:,:,:,n2] = weights_old[0][:,:,:,n1]\n",
    "\n",
    "        # change channels n1 and n2 in layer 1 (== weights[2], as weights[1] is bias for first layer)\n",
    "        weights_new[2][:,:,n1,:] = weights_old[2][:,:,n2,:]\n",
    "        weights_new[2][:,:,n2,:] = weights_old[2][:,:,n1,:]\n",
    "\n",
    "    elif layer == 1:\n",
    "        # change kernels n1 and n2 (no need to change biases)\n",
    "        weights_new[2][:,:,:,n1] = weights_old[2][:,:,:,n2]\n",
    "        weights_new[2][:,:,:,n2] = weights_old[2][:,:,:,n1]\n",
    "\n",
    "        # change order of weights going to first dense layer (shape = (neurons_old, neurons_new))\n",
    "        block_size = int(np.array(weights_old[4]).shape[0]/np.array(weights_old[2]).shape[3])\n",
    "        weights_new[4][n1*block_size:n1*block_size+block_size-1, :] = weights_old[4][n2*block_size:n2*block_size+block_size-1, :]\n",
    "        weights_new[4][n2*block_size:n2*block_size+block_size-1, :] = weights_old[4][n1*block_size:n1*block_size+block_size-1, :]\n",
    "\n",
    "    # first dense layer\n",
    "    elif layer == 2:\n",
    "        # change neurons n1 and n2 (no need to change biases)\n",
    "        weights_new[4][:,n1] = weights_old[4][:,n2]\n",
    "        weights_new[4][:,n2] = weights_old[4][:,n1]\n",
    "\n",
    "        # change order of weights going to next dense layer (shape = (neurons_old, neurons_new))\n",
    "        weights_new[6][n1, :] = weights_old[6][n2, :]\n",
    "        weights_new[6][n2, :] = weights_old[6][n1, :]\n",
    "\n",
    "    # second dense layer\n",
    "    elif layer == 3:\n",
    "        # change neurons n1 and n2 (no need to change biases)\n",
    "        weights_new[6][:,n1] = weights_old[6][:,n2]\n",
    "        weights_new[6][:,n2] = weights_old[6][:,n1]\n",
    "\n",
    "        # change order of weights going to next dense layer (shape = (neurons_old, neurons_new))\n",
    "        weights_new[8][n1, :] = weights_old[8][n2, :]\n",
    "        weights_new[8][n2, :] = weights_old[8][n1, :]\n",
    "\n",
    "\n",
    "    mod2_new.set_weights(weights_new)\n",
    "    return mod2_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## local search (simple climbing (downwards) to begin with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaSD_local_search(mod1, mod2):\n",
    "    # start with beginning solution (the way it is)\n",
    "    dist = getScoreModels(mod1, mod2)\n",
    "\n",
    "    for i in range(100): ### change stop condition here\n",
    "        \n",
    "        # each few steps, print dist to see how it goes\n",
    "        if i %10 == 0:\n",
    "            print(\"i =\", i, \", dist =\", dist)\n",
    "            \n",
    "        # go to random neighbour and look if it makes score better\n",
    "        # choose random layer with probability relative to neurons in layer \n",
    "        # (total of 64+64+256+256 = 640 neurons)\n",
    "        # last layer (output) not possible to change\n",
    "        layer = np.random.choice([0,1,2,3], p = [1,0,0,0])#[0.1, 0.1, 0.4, 0.4])\n",
    "        #depending on layer, different possible nrs of neurons to choose from\n",
    "        if layer < 2:\n",
    "            n1 = np.random.randint(64)\n",
    "            n2 = np.random.randint(64)\n",
    "        else:\n",
    "            n1 = np.random.randint(256)\n",
    "            n2 = np.random.randint(256)\n",
    "        if n1 != n2:\n",
    "            mod2_new = change_neurons(mod2, layer = layer, n1 = n1, n2 = n2)\n",
    "            dist_new = getScoreModels(mod1, mod2_new)\n",
    "            if np.mean(dist_new) < np.mean(dist):\n",
    "                dist = dist_new\n",
    "                mod2 = mod2_new\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first-layer-first SaSD (with real change of mod2!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareLayers(layer1, layer2):\n",
    "    \n",
    "    assert layer1.shape == layer2.shape\n",
    "\n",
    "    # k is nr of kernels/neurons\n",
    "    k = layer1.shape[-1]\n",
    "\n",
    "    #print(layer1.shape)\n",
    "    bottomList1 = [] \n",
    "    bottomList2 = []\n",
    "    \n",
    "    # if conv layer, shape has length 4 (height, width, channels, kernels)\n",
    "    if len(layer1.shape) == 4:\n",
    "      \n",
    "        for kernel in range(k):\n",
    "            bottomList1.append(layer1[:,:,:, kernel].flatten())\n",
    "            bottomList2.append(layer2[:,:,:, kernel].flatten())\n",
    "\n",
    "    # a dense layer, shape is (neurons last layer, neurons this layer)\n",
    "    else:\n",
    "        for j in range(layer1.shape[1]):\n",
    "            bottomList1.append(layer1[:, j])\n",
    "            bottomList2.append(layer2[:, j])\n",
    "        \n",
    "        # if last layer (output layer has 10 neurons):\n",
    "        if layer1.shape[1] == 10:\n",
    "            # do just compute distance, without being able to change order of output neurons\n",
    "            summed_dist = 0\n",
    "            for j in range(10):\n",
    "                summed_dist += editDistanceSigns(bottomList1[j], bottomList2[j])\n",
    "            return summed_dist/10, range(10), range(10)     \n",
    "\n",
    "    editMatrix = np.zeros((k, k))\n",
    "    for j1 in range(k):\n",
    "        for j2 in range(k):\n",
    "            editMatrix[j1, j2] = editDistanceSigns(bottomList1[j1], bottomList2[j2])\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(editMatrix)\n",
    "    minCost = editMatrix[row_ind, col_ind].sum()\n",
    "\n",
    "    return minCost / k, row_ind, col_ind\n",
    "\n",
    "\n",
    "\n",
    "def compareModels(mod1, mod2):\n",
    "    \n",
    "    # get array of weights of conv and dense layers\n",
    "    weightsNN1 = getWeightsArray(mod1)\n",
    "    weightsNN2 = getWeightsArray(mod2)\n",
    "    \n",
    "    # get weights and biases from mod2, length 10 (5 layers, weights and biases seperately)\n",
    "    weightsNN2_new = mod2.get_weights()\n",
    "    \n",
    "    # boolean is set to True if we have a conv layer and remains as such until we reach first Dense layer\n",
    "    # it recognizes this and is set to False thereafter\n",
    "    firstDenseAfterConv = False\n",
    "    # saves length of last conv layer before first dense\n",
    "    lastConvLen = 0\n",
    "            \n",
    "    numLayers = len(weightsNN1)\n",
    "    assert len(weightsNN2) == numLayers\n",
    "    \n",
    "    editDistance = np.zeros(numLayers)\n",
    "    \n",
    "    # for first layer: compare them and return new order of NN2-neurons/kernels \n",
    "    k = 0\n",
    "    layerNN1 = weightsNN1[k].copy()\n",
    "    layerNN2 = weightsNN2[k].copy()\n",
    "    editDistance[k], hid_layerNN1, hid_layerNN2 = compareLayers(layerNN1, layerNN2)\n",
    "        \n",
    "    ### added to actually save model weights of mod2\n",
    "    for j in range(np.array(weightsNN2_new[2*k]).shape[-1]):\n",
    "        weightsNN2_new[2*k][:,:,:,j] = weightsNN2[k][:,:,:,hid_layerNN2[j]].copy()\n",
    "    ###\n",
    "    \n",
    "    # if first layer is Conv, we have to make the transition when first dense layer is ahead\n",
    "    if len(layerNN1.shape) == 4:\n",
    "        firstDenseAfterConv = True\n",
    "    \n",
    "    # for all other layers:\n",
    "    for k in range(1, numLayers):\n",
    "        layerNN1 = weightsNN1[k].copy()\n",
    "        layerNN2 = weightsNN2[k].copy()\n",
    "        \n",
    "        # 3 possibilities: conv layer is next, first dense layer, or other dense layers\n",
    "        \n",
    "        # nr 1: we are dealing with a conv layer\n",
    "        if len(layerNN1.shape) == 4:\n",
    "            # iterate through all channels in layer\n",
    "            for j in range(weightsNN2[k].shape[2]):\n",
    "                # reorder channels in kernel\n",
    "                layerNN2[:,:,j,:] = weightsNN2[k][:,:,hid_layerNN2[j],:].copy()\n",
    "                \n",
    "                ### added to actually change weights of mod2\n",
    "                weightsNN2_new[2*k][:,:,j,:] = weightsNN2[k][:,:,hid_layerNN2[j],:].copy()\n",
    "                ###\n",
    "                \n",
    "            # save number of kernels in case it is the last conv layer\n",
    "            lastConvLen = weightsNN2[k].shape[-1]\n",
    "            \n",
    "        # nr 2: first dense layer after having had a conv layer\n",
    "        elif firstDenseAfterConv:\n",
    "            # change order of first dense layer according to hid_layerNN2\n",
    "            block_size = int(layerNN2.shape[0]/lastConvLen)\n",
    "            for i in range(lastConvLen):\n",
    "                layerNN2[i*block_size:i*block_size+block_size-1, :] = weightsNN2[k][hid_layerNN2[i]*block_size:hid_layerNN2[i]*block_size+block_size-1, :].copy()\n",
    "                ### added to actually change weights of mod2\n",
    "                weightsNN2_new[2*k][i*block_size:i*block_size+block_size-1, :] = weightsNN2[k][hid_layerNN2[i]*block_size:hid_layerNN2[i]*block_size+block_size-1, :].copy()\n",
    "                ###\n",
    "                \n",
    "            firstDenseAfterConv = False\n",
    "            \n",
    "        # nr 3: normal dense layer after dense\n",
    "        else:\n",
    "            for j in range(weightsNN2[k].shape[0]):\n",
    "                layerNN2[j, :] = weightsNN2[k][hid_layerNN2[j], :].copy()\n",
    "                \n",
    "                ### added to actually change weights of mod2\n",
    "                weightsNN2_new[2*k][j,:] = weightsNN2[k][hid_layerNN2[j],:].copy()\n",
    "                ###\n",
    "                \n",
    "        editDistance[k], hid_layerNN1, hid_layerNN2 = compareLayers(layerNN1, layerNN2)\n",
    "        \n",
    "        ### added to actually save model weights of mod2\n",
    "        weightsNN2_copy = copy.deepcopy(weightsNN2_new[2*k])\n",
    "        if k < 2:\n",
    "            for j in range(np.array(weightsNN2_new[2*k]).shape[-1]):\n",
    "                weightsNN2_new[2*k][:,:,:,j] = weightsNN2_copy[:,:,:,hid_layerNN2[j]].copy()\n",
    "        else:\n",
    "            for j in range(np.array(weightsNN2_new[2*k]).shape[-1]):\n",
    "                weightsNN2_new[2*k][:,j] = weightsNN2_copy[:,hid_layerNN2[j]].copy()\n",
    "                \n",
    "    mod2_new = getModelFromFile(\"conv2.json\", \"zeros.h5\")\n",
    "    mod2_new.set_weights(weightsNN2_new)\n",
    "    ###\n",
    "            \n",
    "        \n",
    "    return editDistance, mod2_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use both approaches (take greedy solution and try to get better with local search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaSD_local_search_plus_first(mod1, mod2):\n",
    "    # start with beginning solution (take first-layer-first method)\n",
    "    dist, mod2 = compareModels(mod1, mod2)\n",
    "    \n",
    "    print(\"orig dist is\", dist)\n",
    "\n",
    "    for i in range(3000): # change for other stopping criterion\n",
    "        # every 100 steps print momentary best dist\n",
    "        if i %100 == 0:\n",
    "            print(dist)\n",
    "            \n",
    "        # go to random neighbour and look if it makes score better\n",
    "        # choose random layer with probability relative to neurons in layer \n",
    "        # (total of 64+64+256+256 = 640 neurons)\n",
    "        # last layer (output) not possible to change\n",
    "        layer = np.random.choice([0,1,2,3], p = [0.1, 0.1, 0.4, 0.4])\n",
    "        \n",
    "        #depending on layer, different possible nrs of neurons to choose from\n",
    "        if layer < 2:\n",
    "            n1 = np.random.randint(64)\n",
    "            n2 = np.random.randint(64)\n",
    "        else:\n",
    "            n1 = np.random.randint(256)\n",
    "            n2 = np.random.randint(256)\n",
    "            \n",
    "        # if its not the same neuron, change them and compute score\n",
    "        if n1 != n2:\n",
    "            mod2_new = change_neurons(mod2, layer = layer, n1 = n1, n2 = n2)\n",
    "            dist_new = getScoreModels(mod1, mod2_new)\n",
    "            # if score is better than the one before, go to this neighbour and continue from here\n",
    "            if np.mean(dist_new) <= np.mean(dist):\n",
    "                dist = dist_new\n",
    "                mod2 = mod2_new\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get CNN from json and h5 files\n",
    "def getModelFromFile(json_file, h5_file):\n",
    "    # get model structure from json\n",
    "    json = open(json_file, \"r\")\n",
    "    loaded_json = json.read()\n",
    "    json.close()\n",
    "    model = model_from_json(loaded_json)\n",
    "    \n",
    "    # load weights in model\n",
    "    model.load_weights(h5_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_WT(his_WT, his_orig):\n",
    "    return (np.argmin(his_WT[\"val_loss\"])<=np.argmin(his_orig[\"val_loss\"])) and (np.min(his_WT[\"val_loss\"])<1.02*np.min(his_orig[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array with WTs and with random tickets\n",
    "WTs_CIFAR = []\n",
    "WTs_CINIC = []\n",
    "WTs_SVHN = []\n",
    "randoms = []\n",
    "\n",
    "# for each possible WT for CIFAR, add to array if it is one\n",
    "for i in range(0, 20):\n",
    "    # extract history\n",
    "    his_orig = pickle.load(open('../tickets/WTs_CIFAR/his_orig_s0.1_nr' + str(i), \"rb\"))\n",
    "    his_WT = pickle.load(open('../tickets/WTs_CIFAR/his_WT_s0.1_nr' + str(i), \"rb\"))\n",
    "    # check if it is a WT (min epoch same or equal, min val_loss smaller or only 2%(?) higher)\n",
    "    if is_WT(his_WT, his_orig):\n",
    "        WTs_CIFAR.append(getModelFromFile(\"../tickets/conv2.json\", \"../tickets/WTs_CIFAR/WT_s0.1_nr\" + str(i) + \".h5\"))\n",
    "\n",
    "#CINIC\n",
    "for i in range(0, 20):\n",
    "    # extract history\n",
    "    his_orig = pickle.load(open('../tickets/WTs_CINIC/his_orig_s0.1_nr' + str(i), \"rb\"))\n",
    "    his_WT = pickle.load(open('../tickets/WTs_CINIC/his_WT_s0.1_nr' + str(i), \"rb\"))\n",
    "    # check if it is a WT (min epoch same or equal, min val_loss smaller or only 2%(?) higher)\n",
    "    if is_WT(his_WT, his_orig):\n",
    "        WTs_CINIC.append(getModelFromFile(\"../tickets/conv2.json\", \"../tickets/WTs_CINIC/WT_s0.1_nr\" + str(i) + \".h5\"))\n",
    "        \n",
    "#SVHN\n",
    "for i in range(0, 20):\n",
    "    # extract history\n",
    "    his_orig = pickle.load(open('../tickets/WTs_SVHN/his_orig_s0.1_nr' + str(i), \"rb\"))\n",
    "    his_WT = pickle.load(open('../tickets/WTs_SVHN/his_WT_s0.1_nr' + str(i), \"rb\"))\n",
    "    # check if it is a WT (min epoch same or equal, min val_loss smaller or only 2%(?) higher)\n",
    "    if is_WT(his_WT, his_orig):\n",
    "        WTs_SVHN.append(getModelFromFile(\"../tickets/conv2.json\", \"../tickets/WTs_SVHN/WT_s0.1_nr\" + str(i) + \".h5\"))\n",
    "        \n",
    "        \n",
    "# for each random ticket, add to array if it is not a WT\n",
    "for i in range(0,20):\n",
    "    # extract history\n",
    "    his_orig = pickle.load(open('../tickets/random/his_orig_s0.1_nr' + str(i), \"rb\"))\n",
    "    his_random = pickle.load(open('../tickets/random/his_random_s0.1_nr' + str(i), \"rb\"))\n",
    "    # check if it is a WT (min epoch same or equal, min val_loss smaller or only 2%(?) higher)\n",
    "    if not is_WT(his_random, his_orig):\n",
    "        randoms.append(getModelFromFile(\"../tickets/conv2.json\", \"../tickets/random/random_s0.1_nr\" + str(i) + \".h5\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0 , dist = [0.91676019 0.90696023 0.97946739 0.97484517 0.9744076 ]\n",
      "i = 10 , dist = [0.90596834 0.90667595 0.97946739 0.97484517 0.9744076 ]\n",
      "i = 20 , dist = [0.88724792 0.90785737 0.97946739 0.97484517 0.9744076 ]\n",
      "i = 30 , dist = [0.87565735 0.9078186  0.97946739 0.97484517 0.9744076 ]\n",
      "i = 40 , dist = [0.8739515  0.90591506 0.97946739 0.97484517 0.9744076 ]\n",
      "i = 50 , dist = [0.85855347 0.90639821 0.97946739 0.97484517 0.9744076 ]\n",
      "i = 60 , dist = [0.85060135 0.90539502 0.97946739 0.97484517 0.9744076 ]\n",
      "i = 70 , dist = [0.84383092 0.90454424 0.97946739 0.97484517 0.9744076 ]\n",
      "i = 80 , dist = [0.83971199 0.90468196 0.97946739 0.97484517 0.9744076 ]\n",
      "i = 90 , dist = [0.8368128  0.90580851 0.97946739 0.97484517 0.9744076 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.8368128 , 0.90580851, 0.97946739, 0.97484517, 0.9744076 ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can start comparing!\n",
    "# Only compare WTs_CIFAR 0 and 1 for a test period of 100 iterations (far to few, but takes already some time)\n",
    "SaSD_local_search(WTs_CIFAR[0], WTs_CIFAR[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
