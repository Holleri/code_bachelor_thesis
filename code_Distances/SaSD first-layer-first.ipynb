{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SaSD to compare (Convolutional) Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeightsArray(mod):\n",
    "    weights = []\n",
    "    for layer in mod.layers:\n",
    "        if isinstance(layer, keras.layers.core.Dense) or isinstance(layer, keras.layers.convolutional.Conv2D):\n",
    "            weights.append(np.array(layer.get_weights()[0])) \n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets two vectors with all weights (! and not just the connection indices) and gives back edit distance\n",
    "def editDistanceSigns(a,b):\n",
    "    nums = len(a) - len(np.intersect1d(np.where(b==0), np.where(a==0)))\n",
    "    same = len(np.intersect1d(np.where(a<0), np.where(b<0))) + len(np.intersect1d(np.where(a>0), np.where(b>0)))\n",
    "    if nums == 0:\n",
    "        return 0\n",
    "    return (nums-same)/nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareLayers(layer1, layer2):\n",
    "    \n",
    "    assert layer1.shape == layer2.shape\n",
    "\n",
    "    # k is nr of kernels/neurons\n",
    "    k = layer1.shape[-1]\n",
    "\n",
    "    #print(layer1.shape)\n",
    "    bottomList1 = [] \n",
    "    bottomList2 = []\n",
    "    \n",
    "    # if conv layer, shape has length 4 (height, width, channels, kernels)\n",
    "    if len(layer1.shape) == 4:\n",
    "      \n",
    "        for kernel in range(k):\n",
    "            bottomList1.append(layer1[:,:,:, kernel].flatten())\n",
    "\n",
    "        for kernel in range(k):\n",
    "            bottomList2.append(layer2[:,:,:, kernel].flatten())\n",
    "\n",
    "    # a dense layer, shape is (neurons last layer, neurons this layer)\n",
    "    else:\n",
    "        for j in range(layer1.shape[1]):\n",
    "            bottomList1.append(layer1[:, j])\n",
    "\n",
    "        for j in range(layer2.shape[1]):\n",
    "            bottomList2.append(layer2[:, j])\n",
    "\n",
    "        # if last layer (output layer has 10 neurons):\n",
    "        if layer1.shape[1] == 10:\n",
    "            # do just compute distance, without being able to change order of output neurons\n",
    "            summed_dist = 0\n",
    "            for j in range(10):\n",
    "                summed_dist += editDistanceSigns(bottomList1[j], bottomList2[j])\n",
    "            return summed_dist/10, range(10), range(10)     \n",
    "\n",
    "    editMatrix = np.zeros((k, k))\n",
    "    for j1 in range(k):\n",
    "        for j2 in range(k):\n",
    "            editMatrix[j1, j2] = editDistanceSigns(bottomList1[j1], bottomList2[j2])\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(editMatrix)\n",
    "    minCost = editMatrix[row_ind, col_ind].sum()\n",
    "\n",
    "    return minCost / k, row_ind, col_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareModels(mod1, mod2):\n",
    "    \n",
    "    # get array of weights of conv and dense layers\n",
    "    weightsNN1 = getWeightsArray(mod1)\n",
    "    weightsNN2 = getWeightsArray(mod2)\n",
    "    \n",
    "    # boolean is set to True if we have a conv layer and remains as such until we reach first Dense layer\n",
    "    # it recognizes this and is set to False thereafter\n",
    "    firstDenseAfterConv = False\n",
    "    # saves length of last conv layer before first dense\n",
    "    lastConvLen = 0\n",
    "            \n",
    "    numLayers = len(weightsNN1)\n",
    "    assert len(weightsNN2) == numLayers\n",
    "    \n",
    "    editDistance = np.zeros(numLayers)\n",
    "    \n",
    "    # for first layer: compare them and return new order of NN2-neurons/kernels \n",
    "    k = 0\n",
    "    layerNN1 = weightsNN1[k].copy()\n",
    "    layerNN2 = weightsNN2[k].copy()\n",
    "    editDistance[k], hid_layerNN1, hid_layerNN2 = compareLayers(layerNN1, layerNN2)\n",
    "    \n",
    "    # if first layer is Conv, we have to make the transition when first dense layer is ahead\n",
    "    if len(layerNN1.shape) == 4:\n",
    "        firstDenseAfterConv = True\n",
    "    \n",
    "    # for all other layers:\n",
    "    for k in range(1, numLayers):\n",
    "        layerNN1 = weightsNN1[k].copy()\n",
    "        layerNN2 = weightsNN2[k].copy()\n",
    "        \n",
    "        # 3 possibilities: conv layer is next, first dense layer, or other dense layers\n",
    "        \n",
    "        # nr 1: we are dealing with a conv layer\n",
    "        if len(layerNN1.shape) == 4:\n",
    "            # iterate through all channels in layer\n",
    "            for j in range(weightsNN2[k].shape[-2]):\n",
    "                # reorder channels in kernel\n",
    "                layerNN2[:,:,j,:] = weightsNN2[k][:,:,hid_layerNN2[j],:].copy()\n",
    "            # save number of kernels in case it is the last conv layer\n",
    "            lastConvLen = weightsNN2[k].shape[-1]\n",
    "            \n",
    "        # nr 2: first dense layer after having had a conv layer\n",
    "        elif firstDenseAfterConv:\n",
    "            # change order of first dense layer according to hid_layerNN2\n",
    "            block_size = int(layerNN2.shape[0]/lastConvLen)\n",
    "            for i in range(lastConvLen):\n",
    "                layerNN2[i*block_size:i*block_size+block_size-1, :] = weightsNN2[k][hid_layerNN2[i]*block_size:hid_layerNN2[i]*block_size+block_size-1, :].copy()   \n",
    "            firstDenseAfterConv = False\n",
    "            \n",
    "        # nr 3: normal dense layer after dense\n",
    "        else:\n",
    "            for j in range(weightsNN2[k].shape[0]):\n",
    "                layerNN2[j, :] = weightsNN2[k][hid_layerNN2[j], :].copy()\n",
    "                \n",
    "        editDistance[k], hid_layerNN1, hid_layerNN2 = compareLayers(layerNN1, layerNN2)\n",
    "        \n",
    "    return editDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get CNN from json and h5 files\n",
    "def getModelFromFile(json_file, h5_file):\n",
    "    # get model structure from json\n",
    "    json = open(json_file, \"r\")\n",
    "    loaded_json = json.read()\n",
    "    json.close()\n",
    "    model = model_from_json(loaded_json)\n",
    "    \n",
    "    # load weights in model\n",
    "    model.load_weights(h5_file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_WT(his_WT, his_orig):\n",
    "    return (np.argmin(his_WT[\"val_loss\"])<=np.argmin(his_orig[\"val_loss\"])) and (np.min(his_WT[\"val_loss\"])<1.02*np.min(his_orig[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array with WTs and with random tickets\n",
    "WTs_CIFAR = []\n",
    "WTs_CINIC = []\n",
    "WTs_SVHN = []\n",
    "randoms = []\n",
    "\n",
    "# for each possible WT for CIFAR, add to array if it is one\n",
    "for i in range(0, 20):\n",
    "    # extract history\n",
    "    his_orig = pickle.load(open('../tickets/WTs_CIFAR/his_orig_s0.1_nr' + str(i), \"rb\"))\n",
    "    his_WT = pickle.load(open('../tickets/WTs_CIFAR/his_WT_s0.1_nr' + str(i), \"rb\"))\n",
    "    # check if it is a WT (min epoch same or equal, min val_loss smaller or only 2%(?) higher)\n",
    "    if is_WT(his_WT, his_orig):\n",
    "        WTs_CIFAR.append(getModelFromFile(\"../tickets/conv2.json\", \"../tickets/WTs_CIFAR/WT_s0.1_nr\" + str(i) + \".h5\"))\n",
    "\n",
    "#CINIC\n",
    "for i in range(0, 20):\n",
    "    # extract history\n",
    "    his_orig = pickle.load(open('../tickets/WTs_CINIC/his_orig_s0.1_nr' + str(i), \"rb\"))\n",
    "    his_WT = pickle.load(open('../tickets/WTs_CINIC/his_WT_s0.1_nr' + str(i), \"rb\"))\n",
    "    # check if it is a WT (min epoch same or equal, min val_loss smaller or only 2%(?) higher)\n",
    "    if is_WT(his_WT, his_orig):\n",
    "        WTs_CINIC.append(getModelFromFile(\"../tickets/conv2.json\", \"../tickets/WTs_CINIC/WT_s0.1_nr\" + str(i) + \".h5\"))\n",
    "        \n",
    "#SVHN\n",
    "for i in range(0, 20):\n",
    "    # extract history\n",
    "    his_orig = pickle.load(open('../tickets/WTs_SVHN/his_orig_s0.1_nr' + str(i), \"rb\"))\n",
    "    his_WT = pickle.load(open('../tickets/WTs_SVHN/his_WT_s0.1_nr' + str(i), \"rb\"))\n",
    "    # check if it is a WT (min epoch same or equal, min val_loss smaller or only 2%(?) higher)\n",
    "    if is_WT(his_WT, his_orig):\n",
    "        WTs_SVHN.append(getModelFromFile(\"../tickets/conv2.json\", \"../tickets/WTs_SVHN/WT_s0.1_nr\" + str(i) + \".h5\"))\n",
    "        \n",
    "        \n",
    "# for each random ticket, add to array if it is not a WT\n",
    "for i in range(0,20):\n",
    "    # extract history\n",
    "    his_orig = pickle.load(open('../tickets/random/his_orig_s0.1_nr' + str(i), \"rb\"))\n",
    "    his_random = pickle.load(open('../tickets/random/his_random_s0.1_nr' + str(i), \"rb\"))\n",
    "    # check if it is a WT (min epoch same or equal, min val_loss smaller or only 2%(?) higher)\n",
    "    if not is_WT(his_random, his_orig):\n",
    "        randoms.append(getModelFromFile(\"../tickets/conv2.json\", \"../tickets/random/random_s0.1_nr\" + str(i) + \".h5\"))\n",
    "        \n",
    "# take same amount of subnetworks for each condition (minimum of WTs of each type, in my case 14)  \n",
    "min_len = min(len(randoms), len(WTs_SVHN), len(WTs_CIFAR), len(WTs_CINIC))\n",
    "\n",
    "randoms = randoms[:min_len]\n",
    "WTs_SVHN = WTs_SVHN[:min_len]\n",
    "WTs_CIFAR = WTs_CIFAR[:min_len]\n",
    "WTs_CINIC = WTs_CINIC[:min_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare each group to itself and to all others (10 conditions in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When comparing different tickets, possibility to choose from 14*14 = 196 combinations\n",
    "# When comparing same tickets, possibility to choose from 13+12+...+1 = 91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each group to itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each group of tickets, compare to itself (4 condititions in total)\n",
    "for name, group in zip([\"random\", \"WTs_SVHN\", \"WTs_CINIC\", \"WTs_CIFAR\"],\n",
    "                       [randoms, WTs_SVHN, WTs_CINIC, WTs_CIFAR]):\n",
    "    \n",
    "    print(\"starting group\", name)\n",
    "    \n",
    "    # create array with 30 of 91 of the values True, other ones False and shuffle\n",
    "    bool_arr = np.concatenate((np.ones(30, dtype = bool), np.zeros(61, dtype = bool)))\n",
    "    np.random.shuffle(bool_arr)\n",
    "\n",
    "    s = 0\n",
    "    dists_all_tickets = []\n",
    "\n",
    "    for i,ticket1 in enumerate(group):\n",
    "        for j,ticket2 in enumerate(group):\n",
    "            if i<j:\n",
    "                if bool_arr[s]:\n",
    "                    dists = compareModels(ticket1, ticket2)\n",
    "                    dists_all_tickets.append(dists)\n",
    "                    print(\"distance of ticket\", i, \"and ticket\", j, \"is:\", dists)\n",
    "                    print(\"mean distance:\", np.mean(dists))\n",
    "                s += 1\n",
    "                \n",
    "    np.savetxt(\"../dists/dists_\" + name + \".csv\", dists_all_tickets, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each WT-group to random group (mixed condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each group of tickets, compare to random (3 conditions in total)\n",
    "for name, group in zip([\"SVHN\", \"CINIC\", \"CIFAR\"],\n",
    "                       [WTs_SVHN, WTs_CINIC, WTs_CIFAR]):\n",
    "\n",
    "    print(\"starting group\", name, \"and random\")\n",
    "\n",
    "    # create array with 30 of 196 of the values True, other ones False and shuffle\n",
    "    bool_arr = np.concatenate((np.ones(30, dtype = bool), np.zeros(166, dtype = bool)))\n",
    "    np.random.shuffle(bool_arr)\n",
    "\n",
    "    s = 0\n",
    "    dists_all_tickets = []\n",
    "\n",
    "    for i,ticket1 in enumerate(group):\n",
    "        for j,ticket2 in enumerate(randoms):\n",
    "            if bool_arr[s]:\n",
    "                dists = compareModels(ticket1, ticket2)\n",
    "                dists_all_tickets.append(dists)\n",
    "                print(\"distance of ticket\", i, \"and random\", j, \"is:\", dists)\n",
    "                print(\"mean distance:\", np.mean(dists))\n",
    "            s += 1\n",
    "\n",
    "    np.savetxt(\"../dists/dists_mixed_\" + name + \".csv\", dists_all_tickets, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each WT-group with all other WT-groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each group of WTs, compare to other groups of WTs (3 conditions in total)\n",
    "for name, group in zip([\"SVHN\", \"CINIC\"],\n",
    "                       [WTs_SVHN, WTs_CINIC]):\n",
    "    for name2, group2 in zip([\"CINIC\", \"CIFAR\"],\n",
    "                       [WTs_CINIC, WTs_CIFAR]):\n",
    "        if (name != name2):\n",
    "            print(\"starting group\", name, \"and\", name2)\n",
    "\n",
    "            # create array with 30 of 196 of the values True, other ones False and shuffle\n",
    "            bool_arr = np.concatenate((np.ones(30, dtype = bool), np.zeros(166, dtype = bool)))\n",
    "            np.random.shuffle(bool_arr)\n",
    "\n",
    "            s = 0\n",
    "            dists_all_tickets = []\n",
    "\n",
    "            for i,ticket1 in enumerate(group):\n",
    "                for j,ticket2 in enumerate(group2):\n",
    "                    if bool_arr[s]:\n",
    "                        dists = compareModels(ticket1, ticket2)\n",
    "                        dists_all_tickets.append(dists)\n",
    "                        print(\"distance of ticket\", i, \"and ticket\", j, \"is:\", dists)\n",
    "                        print(\"mean distance:\", np.mean(dists))\n",
    "                    s += 1\n",
    "\n",
    "            np.savetxt(\"../dists/dists_between_\" + name + \"_\" + name2 + \".csv\", \n",
    "                       dists_all_tickets, delimiter = \",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
